{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5a7a5578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "#import cv2\n",
    "from keras import backend as K\n",
    "from keras.layers import Layer,InputSpec\n",
    "import keras.layers as kl\n",
    "from glob import glob\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras import callbacks \n",
    "from  matplotlib import pyplot as plt\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import concatenate,Dense, Conv2D, MaxPooling2D, Flatten,Input,Activation,add,AveragePooling2D,BatchNormalization,Dropout\n",
    "%matplotlib inline\n",
    "import shutil\n",
    "from sklearn.metrics import  precision_score, recall_score, accuracy_score,classification_report ,confusion_matrix\n",
    "from tensorflow.python.platform import build_info as tf_build_info\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf1cbc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Soft Attention\n",
    "\n",
    "from tensorflow.keras import models\n",
    "\n",
    "class SoftAttention(Layer):\n",
    "    def __init__(self,ch,m,concat_with_x=False,aggregate=False,**kwargs):\n",
    "        self.channels=int(ch)\n",
    "        self.multiheads = m\n",
    "        self.aggregate_channels = aggregate\n",
    "        self.concat_input_with_scaled = concat_with_x\n",
    "\n",
    "        \n",
    "        super(SoftAttention,self).__init__(**kwargs)\n",
    "\n",
    "    def build(self,input_shape):\n",
    "\n",
    "        self.i_shape = input_shape\n",
    "\n",
    "        kernel_shape_conv3d = (self.channels, 3, 3) + (1, self.multiheads) # DHWC\n",
    "    \n",
    "        self.out_attention_maps_shape = input_shape[0:1]+(self.multiheads,)+input_shape[1:-1]\n",
    "        \n",
    "        if self.aggregate_channels==False:\n",
    "\n",
    "            self.out_features_shape = input_shape[:-1]+(input_shape[-1]+(input_shape[-1]*self.multiheads),)\n",
    "        else:\n",
    "            if self.concat_input_with_scaled:\n",
    "                self.out_features_shape = input_shape[:-1]+(input_shape[-1]*2,)\n",
    "            else:\n",
    "                self.out_features_shape = input_shape\n",
    "        \n",
    "\n",
    "        self.kernel_conv3d = self.add_weight(shape=kernel_shape_conv3d,\n",
    "                                        initializer='he_uniform',\n",
    "                                        name='kernel_conv3d')\n",
    "        self.bias_conv3d = self.add_weight(shape=(self.multiheads,),\n",
    "                                      initializer='zeros',\n",
    "                                      name='bias_conv3d')\n",
    "\n",
    "        super(SoftAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "\n",
    "        exp_x = K.expand_dims(x,axis=-1)\n",
    "\n",
    "        c3d = K.conv3d(exp_x,\n",
    "                     kernel=self.kernel_conv3d,\n",
    "                     strides=(1,1,self.i_shape[-1]), padding='same', data_format='channels_last')\n",
    "        conv3d = K.bias_add(c3d,\n",
    "                        self.bias_conv3d)\n",
    "        conv3d = kl.Activation('relu')(conv3d)\n",
    "\n",
    "        conv3d = K.permute_dimensions(conv3d,pattern=(0,4,1,2,3))\n",
    "\n",
    "        \n",
    "        conv3d = K.squeeze(conv3d, axis=-1)\n",
    "        conv3d = K.reshape(conv3d,shape=(-1, self.multiheads ,self.i_shape[1]*self.i_shape[2]))\n",
    "\n",
    "        softmax_alpha = K.softmax(conv3d, axis=-1) \n",
    "        softmax_alpha = kl.Reshape(target_shape=(self.multiheads, self.i_shape[1],self.i_shape[2]))(softmax_alpha)\n",
    "\n",
    "        \n",
    "        if self.aggregate_channels==False:\n",
    "            exp_softmax_alpha = K.expand_dims(softmax_alpha, axis=-1)       \n",
    "            exp_softmax_alpha = K.permute_dimensions(exp_softmax_alpha,pattern=(0,2,3,1,4))\n",
    "   \n",
    "            x_exp = K.expand_dims(x,axis=-2)\n",
    "   \n",
    "            u = kl.Multiply()([exp_softmax_alpha, x_exp])   \n",
    "  \n",
    "            u = kl.Reshape(target_shape=(self.i_shape[1],self.i_shape[2],u.shape[-1]*u.shape[-2]))(u)\n",
    "\n",
    "        else:\n",
    "            exp_softmax_alpha = K.permute_dimensions(softmax_alpha,pattern=(0,2,3,1))\n",
    "\n",
    "            exp_softmax_alpha = K.sum(exp_softmax_alpha,axis=-1)\n",
    "\n",
    "            exp_softmax_alpha = K.expand_dims(exp_softmax_alpha, axis=-1)\n",
    "\n",
    "            u = kl.Multiply()([exp_softmax_alpha, x])   \n",
    "\n",
    "        if self.concat_input_with_scaled:\n",
    "            o = kl.Concatenate(axis=-1)([u,x])\n",
    "        else:\n",
    "            o = u\n",
    "        \n",
    "        return [o, softmax_alpha]\n",
    "\n",
    "    def compute_output_shape(self, input_shape): \n",
    "        return [self.out_features_shape, self.out_attention_maps_shape]\n",
    "\n",
    "    \n",
    "    def get_config(self):\n",
    "        return super(SoftAttention,self).get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fd6dd3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_imput(test_path):\n",
    "    datagen=ImageDataGenerator(preprocessing_function=tf.keras.applications.inception_resnet_v2.preprocess_input)\n",
    "    image_size = 299\n",
    "    \n",
    "    test_batch =datagen.flow_from_directory(test_path,\n",
    "                                           target_size=(image_size,image_size),\n",
    "                                           batch_size=1,\n",
    "                                           shuffle=False)\n",
    "    \n",
    "    return test_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "83c43e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(test_batch):\n",
    "    \n",
    "    irv2 = tf.keras.applications.InceptionResNetV2(\n",
    "    include_top=True,\n",
    "    weights=\"imagenet\",\n",
    "    input_tensor=None,\n",
    "    input_shape=None,\n",
    "    pooling=None,\n",
    "    classifier_activation=\"softmax\",\n",
    "\n",
    "    )\n",
    "\n",
    "    # Excluding the last 28 layers of the model.\n",
    "    conv = irv2.layers[-28].output\n",
    "    \n",
    "    attention_layer,map2 = SoftAttention(aggregate=True,m=16,concat_with_x=False,ch=int(conv.shape[-1]),name='soft_attention')(conv)\n",
    "    attention_layer=(MaxPooling2D(pool_size=(2, 2),padding=\"same\")(attention_layer))\n",
    "    conv=(MaxPooling2D(pool_size=(2, 2),padding=\"same\")(conv))\n",
    "\n",
    "    conv = concatenate([conv,attention_layer])\n",
    "    conv  = Activation('relu')(conv)\n",
    "    conv = Dropout(0.5)(conv)\n",
    "    \n",
    "    output = Flatten()(conv)\n",
    "    output = Dense(7, activation='softmax')(output)\n",
    "    model = Model(inputs=irv2.input, outputs=output)\n",
    "    \n",
    "    model.load_weights(\"IRV2+SA.hdf5\")\n",
    "    \n",
    "    predictions = model.predict(test_batch, steps=1, verbose=1)\n",
    "    y_pred = np.argmax(predictions)\n",
    "    targetnames = ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']\n",
    "    \n",
    "    return targetnames[y_pred]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "563ad831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 images belonging to 1 classes.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.compat.v1.nn.convolution_9), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'soft_attention/kernel_conv3d:0' shape=(192, 3, 3, 1, 16) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (tf.nn.bias_add_9), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'soft_attention/bias_conv3d:0' shape=(16,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "1/1 [==============================] - 3s 3s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'bcc'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_path = './predict_img'\n",
    "\n",
    "#预测文件存放位置‘./predict_img/unpredict/test.jpg’\n",
    "\n",
    "test_batch=pre_imput(test_path)\n",
    "\n",
    "predict_image(test_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407ef212",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
